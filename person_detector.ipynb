{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Detector #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is used to find the person in the image so that the person can be cropped out and be used in the project. This will create a more accurate result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from yolov3.utils import Load_Yolo_model, image_preprocess, postprocess_boxes, nms\n",
    "from yolov3.configs import *\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import images ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pose\\\\reference_front\\\\a_hidari_gedan_barai.png', 'pose\\\\reference_front\\\\b_migi_chudan_oi_zuki.png', 'pose\\\\reference_front\\\\c_migi_gedan_barai.png', 'pose\\\\reference_front\\\\d_migi_tetsui_uchi.png', 'pose\\\\reference_front\\\\e_hidari_chudan_oi_zuki.png', 'pose\\\\reference_front\\\\f_hidari_gedan_barai.png', 'pose\\\\reference_front\\\\g_migi_jodan_age_uke.png', 'pose\\\\reference_front\\\\h_hidari_age_uke_jodan.png', 'pose\\\\reference_front\\\\i_migi_jodan_age_uke.png', 'pose\\\\reference_front\\\\j_hidari_gedan_barai.png', 'pose\\\\reference_front\\\\k_migi_chudan_oi_zuki.png', 'pose\\\\reference_front\\\\m_hidari_chudan_oi_zuki.png', 'pose\\\\reference_front\\\\n_hidari_gedan_barai.png', 'pose\\\\reference_front\\\\o_migi_chudan_oi_zuki.png', 'pose\\\\reference_front\\\\p_hidari_chudan_oi_zuki.png', 'pose\\\\reference_front\\\\q_migi_chudan_oi_zuki.png', 'pose\\\\reference_front\\\\r_hidari_chudan_shuto_uke.png', 'pose\\\\reference_front\\\\s_migi_chudan_shuto_uke.png', 'pose\\\\reference_front\\\\t_migi_chudan_shuto_uke.png', 'pose\\\\reference_front\\\\u_hidari_chudan_shuto_uke.png', 'pose\\\\reference_front\\\\v_yame_hachiji_dachi.png', 'pose\\\\reference_front\\\\z.png']\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "images_path = os.path.join(\"pose\", \"reference_front\")\n",
    "\n",
    "for image in os.listdir(images_path):\n",
    "    f = os.path.join(images_path, image)\n",
    "\n",
    "    if os.path.isfile(f):\n",
    "        images.append(f)\n",
    "        \n",
    "print(images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop image function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image, bboxes):\n",
    "    for bbox in bboxes:\n",
    "        coor = np.array(bbox[:4], dtype=np.int32)\n",
    "        class_ind = int(bbox[5])\n",
    "        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
    "\n",
    "        # crop image if person\n",
    "        if class_ind == 0:\n",
    "            cropped_image = image[y1-50:y2+50, x1-50:x2+50]\n",
    "            return cropped_image\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_crop_image(Yolo, image_path, output_path, input_size=416, show=False, score_threshold=0.3, iou_threshold=0.45):\n",
    "    original_image = cv2.imread(image_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image_data = image_preprocess(np.copy(original_image), [\n",
    "                                  input_size, input_size])\n",
    "    image_data = image_data[np.newaxis, ...].astype(np.float32)\n",
    "\n",
    "    if YOLO_FRAMEWORK == \"tf\":\n",
    "        pred_bbox = Yolo.predict(image_data)\n",
    "    elif YOLO_FRAMEWORK == \"trt\":\n",
    "        batched_input = tf.constant(image_data)\n",
    "        result = Yolo(batched_input)\n",
    "        pred_bbox = []\n",
    "        for key, value in result.items():\n",
    "            value = value.numpy()\n",
    "            pred_bbox.append(value)\n",
    "\n",
    "    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n",
    "    pred_bbox = tf.concat(pred_bbox, axis=0)\n",
    "\n",
    "    bboxes = postprocess_boxes(\n",
    "        pred_bbox, original_image, input_size, score_threshold)\n",
    "    bboxes = nms(bboxes, iou_threshold, method='nms')\n",
    "\n",
    "    image = crop(original_image, bboxes)\n",
    "    # CreateXMLfile(\"XML_Detections\", str(int(time.time())), original_image, bboxes, read_class_names(CLASSES))\n",
    "\n",
    "    if output_path != '':\n",
    "        cv2.imwrite(output_path, image)\n",
    "    if show:\n",
    "        # Show the image\n",
    "        cv2.imshow(\"Predicted image\", image)\n",
    "        # Load and hold the image\n",
    "        cv2.waitKey(0)\n",
    "        # To close the window after the required kill value was provided\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load yolo model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = Load_Yolo_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect and crop images ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 614ms/step\n",
      "1/1 [==============================] - 1s 681ms/step\n",
      "1/1 [==============================] - 1s 617ms/step\n",
      "1/1 [==============================] - 1s 680ms/step\n",
      "1/1 [==============================] - 1s 642ms/step\n",
      "1/1 [==============================] - 1s 629ms/step\n",
      "1/1 [==============================] - 1s 620ms/step\n",
      "1/1 [==============================] - 1s 603ms/step\n",
      "1/1 [==============================] - 1s 645ms/step\n",
      "1/1 [==============================] - 1s 653ms/step\n",
      "1/1 [==============================] - 1s 628ms/step\n",
      "1/1 [==============================] - 1s 592ms/step\n",
      "1/1 [==============================] - 1s 572ms/step\n",
      "1/1 [==============================] - 1s 579ms/step\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 1s 609ms/step\n",
      "1/1 [==============================] - 1s 618ms/step\n",
      "1/1 [==============================] - 1s 627ms/step\n",
      "1/1 [==============================] - 1s 576ms/step\n",
      "1/1 [==============================] - 1s 587ms/step\n",
      "1/1 [==============================] - 1s 577ms/step\n",
      "1/1 [==============================] - 1s 577ms/step\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:783: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images:\n\u001b[0;32m      2\u001b[0m     path \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m)[\u001b[39m2\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m     detect_and_crop_image(yolo, image, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./cropped_pose/reference_front/\u001b[39;49m\u001b[39m{\u001b[39;49;00mpath\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m, show\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[10], line 31\u001b[0m, in \u001b[0;36mdetect_and_crop_image\u001b[1;34m(Yolo, image_path, output_path, input_size, show, score_threshold, iou_threshold)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m# CreateXMLfile(\"XML_Detections\", str(int(time.time())), original_image, bboxes, read_class_names(CLASSES))\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mif\u001b[39;00m output_path \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 31\u001b[0m     cv2\u001b[39m.\u001b[39;49mimwrite(output_path, image)\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m show:\n\u001b[0;32m     33\u001b[0m     \u001b[39m# Show the image\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mPredicted image\u001b[39m\u001b[39m\"\u001b[39m, image)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:783: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n"
     ]
    }
   ],
   "source": [
    "for image in images:\n",
    "    path = image.split('\\\\')[2]\n",
    "    detect_and_crop_image(yolo, image, f'./cropped_pose/reference_front/{path}', show=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karate-trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e7ead78e6b3ff796c6a295dea41d22dbcc7ad8777260a88e925872abe439df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

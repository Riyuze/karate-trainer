{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Detector #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is used to find the person in the image so that the person can be cropped out and be used in the project. This will create a more accurate result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from yolov3.utils import Load_Yolo_model, image_preprocess, postprocess_boxes, nms\n",
    "from yolov3.configs import *\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import images ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./reference_pose/a_hidari_gedan_barai.png', './reference_pose/b_migi_chudan_oi_zuki.png', './reference_pose/c_migi_gedan_barai.png', './reference_pose/d_migi_tetsui_uchi.png', './reference_pose/e_hidari_chudan_oi_zuki.png', './reference_pose/f_hidari_gedan_barai.png', './reference_pose/g_migi_jodan_age_uke.png', './reference_pose/h_hidari_age_uke_jodan.png', './reference_pose/i_migi_jodan_age_uke.png', './reference_pose/j_hidari_gedan_barai.png', './reference_pose/k_migi_chudan_oi_zuki.png', './reference_pose/l_migi_gedan_barai.png', './reference_pose/m_hidari_chudan_oi_zuki.png', './reference_pose/n_hidari_gedan_barai.png', './reference_pose/o_migi_chudan_oi_zuki.png', './reference_pose/p_hidari_chudan_oi_zuki.png', './reference_pose/q_migi_chudan_oi_zuki.png', './reference_pose/r_hidari_chudan_shuto_uke.png', './reference_pose/s_migi_chudan_shuto_uke.png', './reference_pose/t_migi_chudan_shuto_uke.png', './reference_pose/u_hidari_chudan_shuto_uke.png', './reference_pose/v_yame_hachiji_dachi.png']\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "images_path = \"./pose/\"\n",
    "\n",
    "for image in os.listdir(images_path):\n",
    "    f = os.path.join(images_path, image)\n",
    "\n",
    "    if os.path.isfile(f):\n",
    "        images.append(f)\n",
    "        \n",
    "print(images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop image function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image, bboxes):\n",
    "    for bbox in bboxes:\n",
    "        coor = np.array(bbox[:4], dtype=np.int32)\n",
    "        class_ind = int(bbox[5])\n",
    "        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
    "\n",
    "        # crop image if person\n",
    "        if class_ind == 0:\n",
    "            cropped_image = image[y1-50:y2+50, x1-50:x2+50]\n",
    "            return cropped_image\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_crop_image(Yolo, image_path, output_path, input_size=416, show=False, score_threshold=0.3, iou_threshold=0.45):\n",
    "    original_image = cv2.imread(image_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image_data = image_preprocess(np.copy(original_image), [\n",
    "                                  input_size, input_size])\n",
    "    image_data = image_data[np.newaxis, ...].astype(np.float32)\n",
    "\n",
    "    if YOLO_FRAMEWORK == \"tf\":\n",
    "        pred_bbox = Yolo.predict(image_data)\n",
    "    elif YOLO_FRAMEWORK == \"trt\":\n",
    "        batched_input = tf.constant(image_data)\n",
    "        result = Yolo(batched_input)\n",
    "        pred_bbox = []\n",
    "        for key, value in result.items():\n",
    "            value = value.numpy()\n",
    "            pred_bbox.append(value)\n",
    "\n",
    "    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n",
    "    pred_bbox = tf.concat(pred_bbox, axis=0)\n",
    "\n",
    "    bboxes = postprocess_boxes(\n",
    "        pred_bbox, original_image, input_size, score_threshold)\n",
    "    bboxes = nms(bboxes, iou_threshold, method='nms')\n",
    "\n",
    "    image = crop(original_image, bboxes)\n",
    "    # CreateXMLfile(\"XML_Detections\", str(int(time.time())), original_image, bboxes, read_class_names(CLASSES))\n",
    "\n",
    "    if output_path != '':\n",
    "        cv2.imwrite(output_path, image)\n",
    "    if show:\n",
    "        # Show the image\n",
    "        cv2.imshow(\"Predicted image\", image)\n",
    "        # Load and hold the image\n",
    "        cv2.waitKey(0)\n",
    "        # To close the window after the required kill value was provided\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load yolo model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = Load_Yolo_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect and crop images ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 569ms/step\n",
      "1/1 [==============================] - 1s 639ms/step\n",
      "1/1 [==============================] - 1s 693ms/step\n",
      "1/1 [==============================] - 1s 728ms/step\n",
      "1/1 [==============================] - 1s 747ms/step\n",
      "1/1 [==============================] - 1s 664ms/step\n",
      "1/1 [==============================] - 1s 663ms/step\n",
      "1/1 [==============================] - 1s 608ms/step\n",
      "1/1 [==============================] - 1s 641ms/step\n",
      "1/1 [==============================] - 1s 679ms/step\n",
      "1/1 [==============================] - 1s 614ms/step\n",
      "1/1 [==============================] - 1s 619ms/step\n",
      "1/1 [==============================] - 1s 607ms/step\n",
      "1/1 [==============================] - 1s 671ms/step\n",
      "1/1 [==============================] - 1s 621ms/step\n",
      "1/1 [==============================] - 1s 611ms/step\n",
      "1/1 [==============================] - 1s 608ms/step\n",
      "1/1 [==============================] - 1s 614ms/step\n",
      "1/1 [==============================] - 1s 635ms/step\n",
      "1/1 [==============================] - 1s 628ms/step\n",
      "1/1 [==============================] - 1s 616ms/step\n",
      "1/1 [==============================] - 1s 619ms/step\n"
     ]
    }
   ],
   "source": [
    "for image in images:\n",
    "    path = image.split('/')[2]\n",
    "    detect_and_crop_image(yolo, image, f'./cropped_pose/{path}', show=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karate-trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e7ead78e6b3ff796c6a295dea41d22dbcc7ad8777260a88e925872abe439df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
